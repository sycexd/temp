{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91888611-0a08-415e-a989-41adb05d27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75308009-bc93-4571-b199-641ef51195c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c964d-3d74-4f3e-8bc4-d733389cac7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad52ffd-0306-4927-8c84-d01cb9e0a3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67a2b9-921c-4f7b-88d3-8c3e6331113f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ea7a1-72bf-43a8-9de0-cb19da70957a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1307779a-44ff-4279-8cc7-f543ec1840eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e9b29f-f2aa-4fbf-96dd-6eda6bb3267b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b00ec8-cf21-4f65-ad53-156bac73bb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cffb85-568c-4c8b-84f3-9e8a87fb1877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324343e-bebb-4419-940b-2473d06cd4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c4f08a-03d1-4e90-aefa-e23cfbfaec6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb1377-d838-4034-bef5-ed9b726bed5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ffc3f-772f-4310-8ad8-8a20c8831802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f785f2-411e-47aa-be39-78de1a448dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1af013-d358-4d10-bb4b-8237bf75012a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f685d35-2327-4e10-9934-9d6168ad906b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab0786b-3a15-4860-a384-f97564e32e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc7ca9a-ec80-4095-8a6c-5239d5dc7cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728972b-341d-44f4-88d6-99b091ba5bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd87e14-2569-4ba6-9fd6-34ca73eb440c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb553631-1c4c-499e-8db2-3bef76fe4236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09044b95-9200-4ecf-be9d-d516b15f468e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a3f80-ba5d-4586-b3e8-2e452d36c778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f60bd2-edbc-48e3-834d-700146248a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11159b29-dad3-4e09-9059-717cae309513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459ef277-3eb2-459e-bfe0-474e466c2053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d5f82-952f-4238-872e-f9a4bcb549cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82dbe34-c46a-44da-884c-c0e4f41cce74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1da4a-1610-41f8-b781-14e50e51f8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a32707-a956-453d-8d53-ad4e8bf9c1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7ec9d-4605-4a3c-9a86-81d24cb815a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37468ae8-55c4-4871-8236-3b37b9664ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c2934-dae9-452b-bce3-fe2d665be4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11f67f-3084-42a8-9b68-4e85291a9dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98806834-02bb-4d91-9228-5977794e3504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeacb339-f7ea-4557-bf0d-8f701417a8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c86863-0349-48a7-99d6-0945459e9caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883cafe-0df7-488f-b476-7e93b46df15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68789f07-6cb1-4d63-b906-cfeb61bbbb18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e7e4e3-ca09-48a8-8320-701be84625c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a3047b-a228-410a-9a51-65b4157032d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0790324c-a748-4f3f-93fd-e3591bda7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec\n",
    "# words -> vectors\n",
    "# eg:  king - man + woman = queen\n",
    "\n",
    "# CBOW ( continuous bag of words ) \n",
    "#  algo to generate vectors from words ( another one is skipgram ) \n",
    "\n",
    "# objective : predict target word from context words ( exact opposite is dont in skipgram ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37ecdbb-9d68-4780-b9e5-d5d9f555a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.utils import to_categorical, pad_sequences\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fffc3eb4-ec13-4ffc-aca3-bbe9dc3784e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    \"Natural Language Processing is a field of Artificial Intelligence.\",\n",
    "    \"Word embeddings help computers understand human language.\",\n",
    "    \"The CBOW model is a part of Word2Vec technique.\",\n",
    "    \"CBOW predicts the target word using surrounding context words.\",\n",
    "    \"Skip Gram is another architecture of Word2Vec.\",\n",
    "    \"Word2Vec is widely used in NLP applications.\",\n",
    "    \"Embedding layers in deep learning are used to represent words.\",\n",
    "    \"CBOW is faster and works better with frequent words.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff4473c-eeab-4051-9843-39c7ccb6b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 50\n",
      "Sample Vocabulary: [('is', 1), ('of', 2), ('cbow', 3), ('word2vec', 4), ('words', 5), ('language', 6), ('a', 7), ('word', 8), ('the', 9), ('used', 10)]\n"
     ]
    }
   ],
   "source": [
    "#Tokenize and build vocabulary\n",
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "# word2id is a dictionary of {word1 : id1 , word2 : id2 ... } \n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0   # manually add a special \"padding\" token with an ID of 0 | used later to make sure all context windows have the same size\n",
    "id2word = {v: k for k, v in word2id.items()} # exact ulta of word2id \n",
    "\n",
    "# Convert sentences into sequences of IDs\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in data]\n",
    "\n",
    "\n",
    "vocab_size = len(word2id) # total number of unique words\n",
    "embed_size = 100 # Dimensionality of our word embeddings. Each word will be represented by a vector of 100 numbers\n",
    "window_size = 2  # Context window size\n",
    "# Key parameter for CBOW. It means we will use 2 words to the left and 2 words to the right of a target word as its context\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "print(\"Sample Vocabulary:\", list(word2id.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2a75a2f-0d2e-429c-aeb6-07a45ea678fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data (context -> target pairs)\n",
    "\n",
    "# This function is the core of the data generation process for CBOW\n",
    "# It slides a \"window\" across each sentence to create pairs of (context words, target word).\n",
    "\n",
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size * 2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word = []            \n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            # pick context (excluding target word)\n",
    "            context_words.append([words[i] \n",
    "                                  for i in range(start, end) \n",
    "                                  if 0 <= i < sentence_length and i != index])\n",
    "            label_word.append(word)\n",
    "\n",
    "            # pad context & one-hot target\n",
    "            x = pad_sequences(context_words, maxlen=context_length)\n",
    "            y = to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8489216f-9620-4e9d-8a31-0b677d5a3d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['natural', 'language', 'is', 'a'] -> Target (Y): processing\n",
      "Context (X): ['language', 'processing', 'a', 'field'] -> Target (Y): is\n",
      "Context (X): ['processing', 'is', 'field', 'of'] -> Target (Y): a\n",
      "Context (X): ['is', 'a', 'of', 'artificial'] -> Target (Y): field\n",
      "Context (X): ['a', 'field', 'artificial', 'intelligence'] -> Target (Y): of\n"
     ]
    }
   ],
   "source": [
    "# Show few examples of context ,  target pairs\n",
    "i = 0\n",
    "for x, y in generate_context_word_pairs(wids, window_size, vocab_size):\n",
    "    if 0 not in x[0]:  # skip padded ones\n",
    "        print(\"Context (X):\", [id2word[w] for w in x[0]], \"-> Target (Y):\", id2word[np.argmax(y[0])])\n",
    "        i += 1\n",
    "        if i == 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb43513-734c-4b56-8646-e29ae5ba302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hemanggs/Library/Python/3.9/lib/python/site-packages/keras/src/layers/core/embedding.py:100: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,050</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │         \u001b[38;5;34m5,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │         \u001b[38;5;34m5,050\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,050</span> (39.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,050\u001b[0m (39.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,050</span> (39.26 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,050\u001b[0m (39.26 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#Build CBOW model\n",
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_shape=(window_size*2,)))\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\n",
    "cbow.add(Dense(vocab_size, activation=\"softmax\"))\n",
    "cbow.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "print(cbow.summary())\n",
    "\n",
    "\n",
    "# Embedding layer : layer is basically the inpput part of CBOW , the sliding window : this layer outputs the W axb matrix\n",
    "#  where a = window size and ,b= size of hidden layer , \n",
    "#  the middle layer needs W axb matrix for each sliding window as we see in architecture.\n",
    "\n",
    "# Embedding Layer: This is the heart of the model. It's a lookup table where the model will learn the vector representation for each word\n",
    "    # input_dim=vocab_size: It knows there are 50 unique words.\n",
    "    # output_dim=embed_size: It will represent each word as a 100-dimensional vector.\n",
    "    # input_shape=(window_size*2,): It expects an input of 4 context words for each sample.\n",
    "\n",
    "\n",
    "#  this is the middle hidden layer : which has size = dimension of vector embedding we want \n",
    "# Lambda Layer: This layer performs the \"Bag-of-Words\" part. It takes the embeddings of the 4 context words \n",
    "# and simply averages them to create a single 100-dimensional context vector. K.mean(x, axis=1) does this averaging.\n",
    "\n",
    "\n",
    "# final output layer , also size of the vocabulary to predict the correct one hot encoded output matrix , via passing through softmax\n",
    "\n",
    "# Dense Layer: This is the final output layer. It takes the single averaged context vector and tries to predict the target word.\n",
    "    # vocab_size: The output has 50 neurons, one for each word in the vocabulary.\n",
    "    # activation=\"softmax\": This activation function converts the output into a probability distribution, \n",
    "    # giving the probability of each word being the correct target\n",
    "\n",
    "# compile: This step configures the model for training.\n",
    "    # loss=\"categorical_crossentropy\": This is the appropriate loss function when your output is a probability distribution (from softmax)\n",
    "    # optimizer=\"adam\": A standard, effective algorithm for updating the model's weights during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c4a0a7-119f-4b67-bde5-03693d39ff24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 261.8962821960449\n",
      "Epoch: 2 Loss: 260.9323539733887\n",
      "Epoch: 3 Loss: 259.1055529117584\n",
      "Epoch: 4 Loss: 257.0529055595398\n",
      "Epoch: 5 Loss: 254.66983246803284\n",
      "Epoch: 6 Loss: 251.8908874988556\n",
      "Epoch: 7 Loss: 248.6899390220642\n",
      "Epoch: 8 Loss: 245.07982182502747\n",
      "Epoch: 9 Loss: 241.10335493087769\n"
     ]
    }
   ],
   "source": [
    "#Train Model\n",
    "for epoch in range(1, 10):  # run fewer epochs for demo\n",
    "    loss = 0.\n",
    "    i = 0\n",
    "    for x, y in generate_context_word_pairs(wids, window_size, vocab_size):\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        i += 1\n",
    "    print(\"Epoch:\", epoch, \"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61a43843-2179-48b8-9c6d-d95890417420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.054321</td>\n",
       "      <td>-0.074739</td>\n",
       "      <td>-0.060069</td>\n",
       "      <td>0.078936</td>\n",
       "      <td>-0.089854</td>\n",
       "      <td>-0.028408</td>\n",
       "      <td>0.097059</td>\n",
       "      <td>0.090719</td>\n",
       "      <td>0.034698</td>\n",
       "      <td>0.029365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011336</td>\n",
       "      <td>-0.003090</td>\n",
       "      <td>0.155461</td>\n",
       "      <td>0.004605</td>\n",
       "      <td>0.068141</td>\n",
       "      <td>-0.115233</td>\n",
       "      <td>0.074386</td>\n",
       "      <td>0.160993</td>\n",
       "      <td>-0.035914</td>\n",
       "      <td>0.043878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbow</th>\n",
       "      <td>-0.093362</td>\n",
       "      <td>0.037589</td>\n",
       "      <td>-0.085925</td>\n",
       "      <td>-0.025712</td>\n",
       "      <td>-0.161978</td>\n",
       "      <td>0.041992</td>\n",
       "      <td>0.171742</td>\n",
       "      <td>0.110891</td>\n",
       "      <td>0.051549</td>\n",
       "      <td>0.245398</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094618</td>\n",
       "      <td>-0.109927</td>\n",
       "      <td>0.142211</td>\n",
       "      <td>-0.250707</td>\n",
       "      <td>0.094942</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.227027</td>\n",
       "      <td>0.115982</td>\n",
       "      <td>-0.042515</td>\n",
       "      <td>-0.106139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word2vec</th>\n",
       "      <td>-0.032512</td>\n",
       "      <td>-0.148713</td>\n",
       "      <td>0.093060</td>\n",
       "      <td>-0.080591</td>\n",
       "      <td>-0.105891</td>\n",
       "      <td>-0.017151</td>\n",
       "      <td>-0.044656</td>\n",
       "      <td>-0.066360</td>\n",
       "      <td>-0.140784</td>\n",
       "      <td>-0.121386</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192467</td>\n",
       "      <td>-0.171513</td>\n",
       "      <td>-0.046460</td>\n",
       "      <td>0.079466</td>\n",
       "      <td>-0.155151</td>\n",
       "      <td>-0.080826</td>\n",
       "      <td>0.048995</td>\n",
       "      <td>0.086592</td>\n",
       "      <td>-0.068815</td>\n",
       "      <td>0.032962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words</th>\n",
       "      <td>-0.058049</td>\n",
       "      <td>-0.091064</td>\n",
       "      <td>-0.014859</td>\n",
       "      <td>0.103756</td>\n",
       "      <td>-0.001048</td>\n",
       "      <td>0.104799</td>\n",
       "      <td>-0.104455</td>\n",
       "      <td>0.096600</td>\n",
       "      <td>0.133464</td>\n",
       "      <td>0.060599</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157250</td>\n",
       "      <td>-0.124791</td>\n",
       "      <td>0.133660</td>\n",
       "      <td>-0.165393</td>\n",
       "      <td>-0.171648</td>\n",
       "      <td>-0.024072</td>\n",
       "      <td>-0.028570</td>\n",
       "      <td>0.025416</td>\n",
       "      <td>0.022717</td>\n",
       "      <td>0.087944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <td>0.043780</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.125315</td>\n",
       "      <td>-0.127571</td>\n",
       "      <td>0.134679</td>\n",
       "      <td>-0.090180</td>\n",
       "      <td>0.038102</td>\n",
       "      <td>-0.116379</td>\n",
       "      <td>-0.050548</td>\n",
       "      <td>0.152756</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070123</td>\n",
       "      <td>0.069304</td>\n",
       "      <td>-0.096214</td>\n",
       "      <td>-0.076106</td>\n",
       "      <td>-0.024002</td>\n",
       "      <td>0.068214</td>\n",
       "      <td>0.143800</td>\n",
       "      <td>-0.057200</td>\n",
       "      <td>0.064325</td>\n",
       "      <td>0.137079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5   \\\n",
       "of       -0.054321 -0.074739 -0.060069  0.078936 -0.089854 -0.028408   \n",
       "cbow     -0.093362  0.037589 -0.085925 -0.025712 -0.161978  0.041992   \n",
       "word2vec -0.032512 -0.148713  0.093060 -0.080591 -0.105891 -0.017151   \n",
       "words    -0.058049 -0.091064 -0.014859  0.103756 -0.001048  0.104799   \n",
       "language  0.043780  0.023858  0.125315 -0.127571  0.134679 -0.090180   \n",
       "\n",
       "                6         7         8         9   ...        90        91  \\\n",
       "of        0.097059  0.090719  0.034698  0.029365  ... -0.011336 -0.003090   \n",
       "cbow      0.171742  0.110891  0.051549  0.245398  ... -0.094618 -0.109927   \n",
       "word2vec -0.044656 -0.066360 -0.140784 -0.121386  ... -0.192467 -0.171513   \n",
       "words    -0.104455  0.096600  0.133464  0.060599  ... -0.157250 -0.124791   \n",
       "language  0.038102 -0.116379 -0.050548  0.152756  ... -0.070123  0.069304   \n",
       "\n",
       "                92        93        94        95        96        97  \\\n",
       "of        0.155461  0.004605  0.068141 -0.115233  0.074386  0.160993   \n",
       "cbow      0.142211 -0.250707  0.094942  0.013233  0.227027  0.115982   \n",
       "word2vec -0.046460  0.079466 -0.155151 -0.080826  0.048995  0.086592   \n",
       "words     0.133660 -0.165393 -0.171648 -0.024072 -0.028570  0.025416   \n",
       "language -0.096214 -0.076106 -0.024002  0.068214  0.143800 -0.057200   \n",
       "\n",
       "                98        99  \n",
       "of       -0.035914  0.043878  \n",
       "cbow     -0.042515 -0.106139  \n",
       "word2vec -0.068815  0.032962  \n",
       "words     0.022717  0.087944  \n",
       "language  0.064325  0.137079  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save trained word embeddings to a file\n",
    "weights = cbow.get_weights()[0] # weights from the embedding layer , these weights are used to get the word embeddings\n",
    "weights = weights[1:]\n",
    "print(weights.shape)\n",
    "\n",
    "\n",
    "# display 5 word's embeddings\n",
    "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46434d75-f446-412b-a8e3-bcaef9608fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar Words: {'deep': ['embedding', 'learning', 'are', 'in', 'layers'], 'cbow': ['model', 'and', 'predicts', 'target', 'field']}\n"
     ]
    }
   ],
   "source": [
    "# demonstrating that the learned embeddings have captured some semantic meaning. \n",
    "\n",
    "#Find similar words using Euclidean distance\n",
    "distance_matrix = euclidean_distances(weights) # calculates the geometric distance between every pair of word vectors.\n",
    "\n",
    "similar_words = {\n",
    "    search: [id2word[idx] for idx in distance_matrix[word2id[search]-1].argsort()[1:6]+1]\n",
    "    for search in [\"deep\", \"cbow\"]\n",
    "}\n",
    "\n",
    "print(\"Similar Words:\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44eaaf6-15a5-431c-9d1f-a18e9d3415e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
